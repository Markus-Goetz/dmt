Distributed max-tree algorithm background:
==========================================

- max-tree is a tree
- trees are special form of graphs (directed, acyclic)
- findings the max-tree is equivalent to iteratively finding the weakly connected components of a graph for each possible gray level descendingly
- weakly connected components can be computed in distributed systems, but there are issues
    * See: 2015, Iverson et al., Evaluation of connected-component labeling algorithms for distributed-memory systems
    * Problems:
        1) excessive memory usage
        2) a lot of extra computation to merge distributed components
        3) linear communication graph of the processors, i.e. with linear processor increase, we proportionally add overhead, eating up parallelization advantages
        4) bad load balancing, i.e. a lot of processors idle while the others work
- To the rescue: 2015, Flick et. al, A Parallel Connectivity Algorithm for de Bruijn Graphs in Metagenomic Applications
    * Distributed weakly connected component labelling algorithm
    * Actually scales with large data sets
    * Core idea:
        - Chunk data equally
        - Identify locally connected components, but do NOT connect yet
        - Instead: generate tuples with connectivity information
            * Let u,v be connected zones
            * Generate the tuples <u,u>, <v,v>, <u,v>, <v,u>
        - Distribute equally (important for load balancing) and sort globally in parallel by first item in tuple
        - Connect partial tuple chains locally (back-link tuple <v,u> prevents having to communication again)
        - Due to sorting, local connectivity chain need only to be merged with exactly ONE element at processor boundary left and right
        - Can be efficiently implemented as reduction or exclusive-scan operation in distributed systems
        
Distributed max_tree algorithm:
===============================

n = total number of pixels
p = number of processors
d = color depth (e.g. 256 colors)
c = a particular gray level
h = pixels in halo zone

- Equally chunk image/n pixels into p subimages, assign each subimage to a distinct processor
- Include a 1-pixel overlap (called halo) zone with subsequent subimage
- Each core: compute local flat zones and generate connection tuples
- Exchange the halo zone with neighboring processor, generate "stitching" tuples
- Sort and distribute tuples equally among all processors
- For each gray level c compute the global weakly connected components
- Canonize the zones on all processors for the current graylevel c
- Simplify all found connected tuples in interval [d,c] to supertuples representing the entire zone (optimization to minimize recomputation)
- Advance to next gray value c-1

// each core, start like berger algorithm
local_image <- load_partial_image(size=n/p)
R <- reverse_bucket_sort(local_image) // buckets are each color in range[0,d-1]
tuples <- {} // empty list

for each bucket in R
    for each p in bucket
        for each n in N(p)
            if color(p) == color(n)
                root <- find_root(n)
                parent(r) <- p
                
    for each p in bucket
        canonize(p)
        if is_root(p)
            tuples.append(<color(p), p, p, p>)
        for each n in N(p)
            if color(p) < color(n)
                rn <- find_root(n)
                rp <- find_root(p)
                
                tuples.append(<color(p), rp, rn, rp>)
                tuples.append(<color(n), rn, rp, rn>)
                
// generate stitching tuples for immediate neighboring processor
send(h, p + 1)
for each index in [0, length(h)]:
    tuples.append(<color(h[i]), h[i], local_image[i], h[i]>)
    tuples.append(<color(h[i]), local_image[i], h[i], local_image[i]>)
    
// compute distributed weak connected components for each bucket
for each bucket in [d,0]
    resolved_tuples <- distributed_flick_weak_connected_components(tuples[d,bucket])
    canonize(local_image, resolved_tuples)
    super_tuples <- merge_zones(resolved_tuples)
    deallocate(tuples[d,bucket])
    tuples[d,bucket] <- super_tuples
            
Possible optimizations:
======================

- Implement the local weakly connected components using GPUs
    * See: 2010, Kalentev et al., Connected component labeling on a 2D grid using CUDA
- Transpose the image initially, so that it is higher than wide
    * Minimizes halo communication memory h required
- Might require to redistribute the super tuples to increase load balance

Advantages:
===========

- Arbitrary size of image possible
- Parallel processing
- Load balanced approach
- Complexity bound on memory, computation and communication
    * Independent of total(!) number of image pixels, i.e. can only generate as many tuples as local pixel in the worst case
    * Memory usage is independent of color depth, contrary to Wilkinsons approach if it were to be directly matched to distributed systems
    * Communication tree fully logarithmic with number of processors, not linear

Disadvantages:
==============

- Added communication and load balancing overhead
- Implies likely slower speed than shared-memory with same amount of cores

Closing thoughts:
=================

- I have not 100% thought it through yey, but it might very well possible that this directly mappable to the tree of shapes
- Reasoning: it starts like berger and just remembers correct connections it would do in serial
- Run on the ToS-interpolated relation U instead of R and generate connectivity tuples for it instead
- Should lead to a correct, distributed ToS

